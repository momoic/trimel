---
title: "Untitled"
output: html_document
date: "2025-08-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(shapviz)
library(ggplot2)
library(caret)
library(survival)
library(survminer)
library(dplyr)
library(gtsummary)
library(data.table)
library(clusterGeneration)
library(devtools)
library(ClusterR)
library(nnet)
library(randomForest)
library(e1071)
library(xgboost)
library(mclust)
library(cluster)
library(kernlab)
library(mboost)
library(treeshap)
library(kernelshap)
library(doFuture)
```

```{r}
data = read.csv("C://PhD work//trimel//artificial_roswell-EOC.csv")[,-1]
set.seed(1)
```


CT features
```{r}
#data <- read.csv("C://PhD work//clin path code//body comp dataset.csv")
weight = read.csv("C://PhD work//clin path code//EOC body comp weight.csv")
```

Weight to BMI
```{r}
comb_data = merge(data, weight, by = "MRN")
comb_data$BMI = comb_data$chemo_start_wgt/comb_data$ht_first_met**2
```

BMI compared to survival categorical
```{r}
for (i in 1:nrow(comb_data)){
    if (comb_data$BMI[[i]] < 18.5) 
      {comb_data$BMI_cat[[i]] = "Underweight"}
    if (comb_data$BMI[[i]] < 25 & comb_data$BMI[[i]]>=18.5) 
      {comb_data$BMI_cat[[i]] = "Healthy Weight"}
    if (comb_data$BMI[[i]] < 30 & comb_data$BMI[[i]]>=25) 
      {comb_data$BMI_cat[[i]] = "Overweight"}
    if (comb_data$BMI[[i]] < 35 & comb_data$BMI[[i]]>=30) 
      {comb_data$BMI_cat[[i]] = "Obesity Class I"}
    if (comb_data$BMI[[i]] < 40 & comb_data$BMI[[i]]>=35) 
     {comb_data$BMI_cat[[i]] = "Obesity Class II"}
    if (comb_data$BMI[[i]]>=40) 
      {comb_data$BMI_cat[[i]] = "Obesity Class III"}
} 

bmi_tab = comb_data %>%
  group_by(BMI_cat) %>%
  summarize(count = n()) 
bmi_tab = bmi_tab[c(4,2,1,5,3,6),]


comb_data$BMI_cat = factor(comb_data$BMI_cat, levels = c("Underweight", "Healthy Weight", "Overweight", "Obesity Class I", "Obesity Class II", "Obesity Class III"))
```


CT adipose measurements to indices
```{r}
comb_data$subq_ht <- comb_data$PRE_SUBQ_1 /(comb_data$ht_first_met)^2
comb_data$vat_ht <- comb_data$PRE_VAT_1 / (comb_data$ht_first_met)^2
comb_data$sma_ht <- comb_data$PRE_SMA_1 / (comb_data$ht_first_met)^2
  
glm <- glm(MSS ~ vat_ht + sma_ht + subq_ht, data = comb_data, family = binomial)
summary(glm)
```

```{r}
comb_data$DMAI <- (glm[[1]][[1]] + glm[[1]][[2]]*comb_data$vat_ht + glm[[1]][[3]]*comb_data$sma_ht + glm[[1]][[4]]*comb_data$subq_ht)
```

SMD
```{r}
comb_data$SMD <- (comb_data$PRE_SMA_1 / comb_data$PRE_IMA_1)
comb_data$BCHI <- comb_data$SMD * comb_data$DMAI
```

```{r}
data = comb_data[,c(2,3,14:19,34)]
data_bmi = comb_data[,c(2,3,14:19,27,34)]
```

```{r}
preProcValues <- preProcess(data, method = c("center", "scale"))
data.ml = predict(preProcValues, data)

preProcValues <- preProcess(data_bmi, method = c("center", "scale"))
data_bmi.ml = predict(preProcValues, data_bmi)
```



```{r}

opt_gmm = Optimal_Clusters_GMM(data.ml, max_clusters = 50, criterion = "BIC",

                              dist_mode = "eucl_dist", seed_mode = "random_subset",

                              km_iter = 10, em_iter = 10, var_floor = 1e-10,

                              plot_data = T)
plot(1:50, opt_gmm)


gmm = GMM(data.ml, 25, dist_mode = "eucl_dist", seed_mode = "random_subset", km_iter = 10,
          em_iter = 10, verbose = F)   
gmm_bmi = GMM(data_bmi.ml, 11, dist_mode = "eucl_dist", seed_mode = "random_subset", km_iter = 10,
          em_iter = 10, verbose = F)   

pr = predict(gmm, newdata = data.ml)

data$Group <- as.factor(pr)
data.ml$Group = as.factor(pr)

pr = predict(gmm_bmi, newdata = data_bmi.ml)
data_bmi$Group = as.factor(pr)
data_bmi.ml$Group = as.factor(pr)
```

```{r}
calc_sig <- function(i, data){
  
  data$Comparison <- NA
  data$Comparison[data$Group != i] <- "NC"
  data$Comparison[data$Group == i] <- "C"
  if (length(unique(data$Comparison)) > 1){
  logrank <- survdiff(Surv(Met_Time, Mets) ~ Comparison, data = data)
  pval <- logrank$pvalue
  if ((logrank$obs[[1]]/nrow(subset(data, Comparison == "C"))) > (logrank$obs[[2]]/nrow(subset(data, Comparison == "NC")))){
    risk <- "HR"
  }
  if ((logrank$obs[[1]]/nrow(subset(data, Comparison == "C"))) < (logrank$obs[[2]]/nrow(subset(data, Comparison == "NC")))){
  risk <- "LR"
  }
  if ((logrank$obs[[1]]/nrow(subset(data, Comparison == "C"))) == (logrank$obs[[2]]/nrow(subset(data, Comparison == "NC")))){
  risk <- NA
  }
  if (pval < 0.05){
    sig <- "S"
  }
  if (pval >= 0.05){
  sig <- "NS"
  }
  if (logrank$obs[[1]] == 0){
    z <- 1
  }
    if (logrank$obs[[1]] > 0){
    z <- 0
  }
  row <- data.frame("Risk" = risk, "Significance" = sig, "Cluster" = i, "Z" = z)
  return(row)
  }
}


data2 <- data.table::rbindlist(lapply(1:25, calc_sig, data))
```

```{r}
inTraining = createDataPartition(data.ml$Age, p = 0.8, list = FALSE)
train_data = data.ml[inTraining,]
test_data = data.ml[-inTraining,]

inTraining = createDataPartition(data_bmi.ml$Age, p = 0.8, list = FALSE)
train_data_bmi = data_bmi.ml[inTraining,]
test_data_bmi = data_bmi.ml[-inTraining,]

fitControl = trainControl(method = "boot", number = 1)
```


```{r}
set.seed(1)
#nn_model = train(Group~., data = train_data, method = "nnet",trControl = fitControl, tuneGrid = expand.grid(size = c(1,5,10), decay = c(0,0.001,0.1)))
#forest_model = train(Group~., data = train_data, method = "rf", trControl = fitControl, tuneGrid = expand.grid(.mtry = c(2:4)))
#svm_model = train(Group~., data = train_data, method = "lssvmRadial", trControl = fitControl)
#xgb_model = train(Group~., data = train_data, method = "xgbLinear", trControl = fitControl, tuneGrid = expand.grid(nrounds = 300, eta = c(0.01, 0.1, 1), alpha = c(0, 1, 10), lambda = c(0, 1, 10)))
#knn_model = train(Group~., data = train_data, method = "knn", trControl = fitControl, tuneGrid = expand.grid(k = c(5,10,15,20,25,30,35,40,45,50)))
forest_model = randomForest(Group~., data = train_data, mtry = 4, ntree = 500)
forest_model_bmi = randomForest(Group~., data = train_data_bmi, mtry = 4, ntree = 500)
```

```{r}
#nn_pred = predict(nn_model, newdata = test_data)
forest_pred = predict(forest_model, newdata = test_data)
#svm_pred = predict(svm_model, newdata = test_data)
#xgb_pred = predict(xgb_model, newdata = test_data)
#knn_pred = predict(knn_model, newdata = test_data)

#caret::confusionMatrix(as.factor(nn_pred), as.factor(test_data$Group))
caret::confusionMatrix(as.factor(forest_pred), as.factor(test_data$Group))
#caret::confusionMatrix(as.factor(svm_pred), as.factor(test_data$Group))
#caret::confusionMatrix(as.factor(xgb_pred), as.factor(test_data$Group))
#caret::confusionMatrix(as.factor(knn_pred), as.factor(test_data$Group))


```

```{r}
# options(doFuture.rng.onMisuse = "ignore")
# registerDoFuture()
# plan(multisession, workers = 4)
```

```{r}
# xvars = colnames(data)
# xvars.new = colnames(data_bmi)
# 
# X = data |>
#   subset(select = xvars)
# fit = forest_model
# shp = permshap(fit, X = data[sample(nrow(X), 100),], bg_X = data[sample(nrow(X), 20),], type = "prob", parallel = TRUE, parallel_args = list(packages = "caret"))
# shp = kernelshap(fit, X = X,  bg_X = data[sample(nrow(X), 200),], type = "prob", parallel = TRUE)
# sv = shapviz(shp)
# sv_importance(sv)
```

```{r}
unified = randomForest.unify(forest_model, data)
unified_bmi = randomForest.unify(forest_model_bmi, data_bmi)

treeshap_res = treeshap(unified, data)
treeshap_res_bmi = treeshap(unified_bmi, data_bmi)

plot_contribution(treeshap_res, obs = 201)
plot_feature_importance(treeshap_res, max_vars = 8)
plot_feature_importance(treeshap_res_bmi, max_vars = 8)
plot_feature_dependence(treeshap_res_bmi, "BCHI")
```


```{r}

high_risk = subset(data, Group %in% subset(data2, Risk == "HR" & Significance == "S")$Cluster)
low_risk = subset(data, Group %in% subset(data2, Z == 1)$Cluster)
at_risk <- setdiff(data, high_risk)
at_risk <- setdiff(at_risk, low_risk)
at_risk$Risk = "At Risk"
high_risk$Risk = "High Risk"
low_risk$Risk = "Low Risk"
risk_data = rbind(low_risk, at_risk, high_risk)
```